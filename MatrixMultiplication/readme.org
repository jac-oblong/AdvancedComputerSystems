#+title: Matrix-Matrix Multiplication

NOTE: Because of the amount of time it takes to multiply 10,000 x 10,000 matrices (well over 3
hours), a size of 5000 x 5000 will be used instead.

In order to measure the effect of different optimizations on matrix multiplication performance,
multiple different test-benches were created using a combination of multi-threading, cache
optimizations, simd, and sparse matrix compression.

To run all tests, see below.

#+begin_src bash
./run.py
#+end_src

To run a specific test, see below. The ~-t~ option specifies the number of threads. ~-s~ enables SIMD.
~-c~ enables cache optimizations. ~--sparse~ enables sparse matrix compression. ~-s1~ and ~-s2~ are the
sparcities for each matrix. ~<size>~ is the size of the square matrix.

#+begin_src bash
g++ main.cpp matrixmult.cpp -o main -Wall -msse3 -O3

./main [-t <n>] [-sc] [--sparse] [-s1 <%>] [-s2 <%>] <size>

perf stat -- ./main <size>
#+end_src

All data in this document was taken from the generated ~log.txt~ file.

* Different Optimization Levels
** Optimizations for Different Sizes
[[file:images/Optimizations_for_500_Matrix_Size.png]]

[[file:images/Optimizations_for_1000_Matrix_Size.png]]

[[file:images/Optimizations_for_2500_Matrix_Size.png]]

[[file:images/Optimizations_for_5000_Matrix_Size.png]]

As can be seen in the graphs above, SIMD and cache optimizations saw the best performance
improvements. SIMD saw consistent improvement regardless of matrix size, while cache
optimizations saw increasing improvement as the matrix size grew, eventually passing
SIMD at 5000 x 5000 matrix size.

Multi-threading rarely saw a significant improvement, except in the case of multi-threading
with cache. This may be due to the added overhead of managing the threads, or some performance
throttling done by the OS because the computer was running on battery.

** Optimizations for Different Sparcities
[[file:images/Optimizations_for_5000_Matrix_Size_With_0%_Sparcity.png]]

[[file:images/Optimizations_for_5000_Matrix_Size_With_1%_Sparcity.png]]

As can be seen in the graphs above, the performance improvements for each set of
optimizations stays consistent across different sparcities. This makes sense because no
sparse matrix compression is being performed in either case.

* Dense-Dense Matrix Multiplication
[[file:images/Dense-Dense_with_1%_Sparcity.png]]

As can be seen in the execution time increases by ~8x between sizes. This matches what
would be expected, as the size doubles and the non-optimized matrix multiplication algorithm
is O(N^3).

* Sparse-Dense Matrix Multiplication
** Sparse-Dense for Different Sizes
[[file:images/Sparse-Dense_with_Matrix_Size_500.png]]

[[file:images/Sparse-Dense_with_Matrix_Size_1000.png]]

[[file:images/Sparse-Dense_with_Matrix_Size_2500.png]]

[[file:images/Sparse-Dense_with_Matrix_Size_5000.png]]

As can be seen in the graphs above, as the sparcity increases, the execution time tends
to decrease. The decrease is not a major amount, but is still significant.

The only outlier to this is the 500 x 500 matrix, which has an increase in execution time
for the 5% sparcity. This may be caused by the added overhead in generating the sparse-matrix.

** Sparse-Dense for Different Sparcities
[[file:images/Sparse-Dense_with_0.1%_Sparcity.png]]

[[file:images/Sparse-Dense_with_1%_Sparcity.png]]

[[file:images/Sparse-Dense_with_5%_Sparcity.png]]

The trends shown above are pretty consistent with what was seen for Dense-Dense Matrix
Multiplication. Notably, however, the execution times are lower than what was seen for
Dense-Dense matrices, as the matrices are being compressed.

* Sparse-Sparse Matrix Multiplication
** Sparse-Sparse for Different Sizes
[[file:images/Sparse-Sparse_with_Matrix_Size_500.png]]

[[file:images/Sparse-Sparse_with_Matrix_Size_1000.png]]

[[file:images/Sparse-Sparse_with_Matrix_Size_2500.png]]

[[file:images/Sparse-Sparse_with_Matrix_Size_5000.png]]

The results in the graphs above show similar trends to the Sparse-Dense results. As the
sparcity increases, the execution time decreases by a small amount. The execution times
are slightly lower if not similar to the Sparse-Dense results, however.

Unlike the Sparse-Dense results, there is no increase in execution time for 5% sparcity when
the matrix size is 500 x 500. This may be because the increase was an outlier in the data.

** Sparse-Sparse for Different Sparcities
[[file:images/Sparse-Sparse_with_0.1%_Sparcity.png]]

[[file:images/Sparse-Sparse_with_1%_Sparcity.png]]

[[file:images/Sparse-Sparse_with_5%_Sparcity.png]]

The trends shown above are pretty consistent with what was seen for Dense-Dense and
Sparse-Dense matrix multiplication. Similarly to the Sparse-Dense results, the execution
time is much lower than what was seen for Dense-Dense matrices. The results are pretty
consistent with the results of Sparse-Dense multiplication.

From this, it can be concluded that if there is any significant amount of sparcity, it will
most always benefit execution time to compress the matrix. That being said, there will also
be a price in the space complexity of the matrix data structure.
